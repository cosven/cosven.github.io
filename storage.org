* Storage
** FAQ
*** read index 请求是什么
应该是 follower 发给 leader，leader 会返回一个  read index。

据 mh 老师描述，tikv 中的 read index 请求有两种，一个是 raft 的，一个是 tikv
一个是 tikv 的，
tikv 的这个会给 tiflash 用。

#+BEGIN_SRC rust
pub struct ReadIndexRequest {
    pub id: Uuid,
    pub cmds: MustConsumeVec<(RaftCmdRequest, Callback<RocksEngine>, Option<u64>)>,
    pub renew_lease_time: Timespec,
    pub read_index: Option<u64>,
    // `true` means it's in `ReadIndexQueue::reads`.
    in_contexts: bool,
}
#+END_SRC

*** raft 一次写入的流程是什么
当 Client 需要写入某个数据的时候，Client 会将操作发送给 Raft Leader，
这个在 TiKV 里面我们叫做 Propose，Leader 会将操作编码成一个 entry，
写入到自己的 Raft Log 里面，这个我们叫做 =Append= 。

Leader 也会通过 Raft 算法将 entry 复制到其他的 Follower 上面，这个我们叫做
=Replicate= 。Follower 收到这个 entry 之后也会同样进行 Append 操作，
顺带告诉 Leader Append 成功。当 Leader 发现这个 entry 已经被大多数节点 Append，
就认为这个 entry 已经是 Committed 的了，然后就可以将 entry 里面的操作解码出来，
执行并且应用到状态机里面，这个我们叫做 =Apply= 。

理解：TiKV 的 Apply 是异步的；Applied Index <= Committed Index。

*** TiKV 处理一个请求的流程
可以参考这篇文档的 Service 部分
https://pingcap.com/blog-cn/tikv-source-code-reading-9/#service

事务类型的请求，比如 prewrite，会通过 sched_txn_command 方法来处理它
（会把它塞到一个队列里面去，具体细节不是特别了解）。

对于 read_index 请求，可以看到有个 read_index 方法来处理。

总的来说，处理各种类型请求的入口都在 tikv/src/server/service/kv.rs 这个文件中。

*** QUESTION [#C] Region Merge 的条件之一
不是很懂这个。

https://github.com/tikv/tikv/pull/8005
它修复的问题：以前，region merge 允许 target peers 不全部存在（这个存在的意思应该是指分区等）。
但是有些情况，最好所有的 target peers 都在。

比如：一个 target peer 不存在，一个 source peer 等待它被创建。但其间，
其它 target peers 进行了 conf change，把之前的 target peer 给移除了（…）。
然后，这个 region  被 merge 到了另外一个 target region。
在这个场景里面，之前那个等待的 source peer 就不知道自己是否该被移除了。

这个 PR 给 region merge 加了个约束，所有的 target region 都必须存在。这样逻辑更简单。

疑问：怎样判断一个 target peer 是否存在？

** IO 基准测试
这篇文档有使用 fio 测试磁盘性能的方法。
https://cloud.google.com/compute/docs/disks/benchmarking-pd-performance
1. 测试裸盘性能时，filename 要指定为磁盘，filesize 指定为磁盘容量大小。
   而测试文件系统时，这两项配置则比较自由。可以改为文件和 10G 等。
2. 测试 IOPS/带宽/latency 时，都有一些值得注意的参数配置。
   比如测带宽用顺序写，blocksize 调大一点，并发和 iodepth 都调大，
   并且注意不要让其它指标成为瓶颈。

这篇文档介绍了 iodepth 的含义，以及它与 numjobs 的区别。
https://www.spinics.net/lists/fio/msg07191.html
有趣的几点：
1. 一个核有可能只能承受一定的 iodepth。
2. Linux Buffered IO 不是异步的 [[https://fio.readthedocs.io/en/latest/fio_man.html#cmdoption-arg-iodepth][ref]]。

这篇文章介绍了 Linux IO 的基本分层。
https://zhuanlan.zhihu.com/p/71149410
感觉问题主要是两个
1. 知道有多少层缓存。
2. 知道这个图 [[https://pic4.zhimg.com/80/v2-387d87592d876ec23e0774f7d14d8063_1440w.jpg][io layer]]。

*** TiKV

tikv 使用的 rocksdb 是以 buffered IO 为入口往下写的。rocksdb 写入可以认为是阻塞的（by 明华老师）。
所以模拟 tikv 基准测试时，可以用 iodepth=2 或者 iodepth=1,numjobs=2（我的推论）。

** 关于存储的经验值
*** 磁盘指标
0. 盘的性能指标：带宽；IOPS；latency。
1. nvme 盘的 disk latency 一般在 1ms 以下。
2. 观察 TiDB 发版性能测试的指标可以发现在 sysbench oltp_insert 负载中压力下，
   磁盘 avg latency 在 30us 左右。数据导入阶段磁盘 avg latency 在 5ms 左右。

*** RocksDB(tikv) 指标
1. 关于 rocskdb bytes_per_write:
   - 发版测试中，sysbench insert 负载下，kv rocksdb 的 avg bytes_per_write
     为 28.5KB，p99 有 51KB。
   - go-ycsb batchsize 为 128 的时候，bytes_per_write avg 为 300KB，
     p99 有 558KB (这个值只是一次测试的结果)。查看代码发现 value size 为 1k。
     这个我猜应该比 oltp_insert 的 value size 要大。

** RocksDB Rate-Limiter

- 功能：rate limiter
- 作用：限制最大的写入速度
- 场景：写入达到一定程度的时候，容易给读的 latency 带来尖刺
- 介绍：
  - 只对 flush 和 compaction 有影响，对 wal 就不会有影响


- 功能：auto-tuned rate limiter
- 算法：令牌桶算法
- 作用：rate_bytes_per_sec 比较难配置，太低容易导致 memtable 和 L0 文件堆积，
  太高容易影响前台读取和写入。rocksdb 提供算法自动设置这个值。


- 测试点
  - 关于限流
    - 流量一直很高或很低：似乎没问题
    - 流量从高变低：
    - 流量从低变高：
    - 流量过高时，解除限流
  - auto-tuned 开关 + rate-bytes-per-sec 大小
  - flush 和 compaction 流量
  - 人肉指定的 rate-bytes-per-sec
